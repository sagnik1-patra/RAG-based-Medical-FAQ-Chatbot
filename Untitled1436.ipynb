{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b127d7-021d-4cd2-9132-681e6cc308a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading: C:\\Users\\sagni\\Downloads\\RAG based Medical FAQ Chatbot\\archive\\train.csv\n",
      "[INFO] Loaded shape: (16407, 3)\n",
      "[WARN] Could not find ground-truth or prediction columns by common names.\n",
      "[HINT] Ensure your CSV has columns like:\n",
      "  GT  : ['label', 'target', 'answer', 'ground_truth', 'gt']\n",
      "  Pred: ['pred', 'prediction', 'predicted', 'response', 'pred_label']\n",
      "[INFO] Retrieval columns not found; skipping Recall@K (optional).\n",
      "\n",
      "✅ Done. Check your output folder:\n",
      "C:\\Users\\sagni\\Downloads\\RAG based Medical FAQ Chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Paths (EDIT if needed)\n",
    "# ==============================\n",
    "INPUT_CSV  = r\"C:\\Users\\sagni\\Downloads\\RAG based Medical FAQ Chatbot\\archive\\train.csv\"   # or your predictions CSV\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\RAG based Medical FAQ Chatbot\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================\n",
    "# Column name candidates\n",
    "# ==============================\n",
    "GT_CANDIDATES   = [\"label\", \"target\", \"answer\", \"ground_truth\", \"gt\"]\n",
    "PRED_CANDIDATES = [\"pred\", \"prediction\", \"predicted\", \"response\", \"pred_label\"]\n",
    "\n",
    "# Retrieval columns (optional)\n",
    "QUERY_COL_CAND   = [\"query\", \"question\"]\n",
    "GOLD_ID_CAND     = [\"gold_id\", \"relevant_id\", \"doc_id\", \"answer_id\"]\n",
    "RETR_IDS_CAND    = [\"retrieved_ids\", \"retrieved\", \"neighbors\", \"ctx_ids\"]\n",
    "\n",
    "# ==============================\n",
    "# Load data\n",
    "# ==============================\n",
    "print(f\"[INFO] Loading: {INPUT_CSV}\")\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(f\"[INFO] Loaded shape: {df.shape}\")\n",
    "\n",
    "def find_col(candidates, dfcols):\n",
    "    for c in candidates:\n",
    "        if c in dfcols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "cols_lower = {c.lower(): c for c in df.columns}\n",
    "# Map to lowercase to allow flexible matching\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "gt_col   = find_col(GT_CANDIDATES, df.columns)\n",
    "pred_col = find_col(PRED_CANDIDATES, df.columns)\n",
    "\n",
    "if gt_col is None or pred_col is None:\n",
    "    print(\"[WARN] Could not find ground-truth or prediction columns by common names.\")\n",
    "    print(\"[HINT] Ensure your CSV has columns like:\")\n",
    "    print(f\"  GT  : {GT_CANDIDATES}\")\n",
    "    print(f\"  Pred: {PRED_CANDIDATES}\")\n",
    "    # We’ll attempt a graceful exit with message\n",
    "    # (You can adapt this to your actual column names.)\n",
    "else:\n",
    "    # ==============================\n",
    "    # Clean + align labels\n",
    "    # ==============================\n",
    "    def normalize(x):\n",
    "        # unify strings; keep numbers as strings too\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        return str(x).strip()\n",
    "\n",
    "    y_true = df[gt_col].map(normalize).values\n",
    "    y_pred = df[pred_col].map(normalize).values\n",
    "\n",
    "    # Remove rows with empty gt/pred\n",
    "    mask = (y_true != \"\") & (y_pred != \"\")\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"No valid rows after filtering empty ground-truth/prediction.\")\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    # ==============================\n",
    "    # Accuracy\n",
    "    # ==============================\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"[OK] Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # ==============================\n",
    "    # Classification report\n",
    "    # ==============================\n",
    "    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    rep_df = pd.DataFrame(report_dict).transpose()\n",
    "    rep_csv = os.path.join(OUTPUT_DIR, \"classification_report.csv\")\n",
    "    rep_df.to_csv(rep_csv, index=True, encoding=\"utf-8\")\n",
    "    print(f\"[OK] Saved classification report -> {rep_csv}\")\n",
    "\n",
    "    # ==============================\n",
    "    # Confusion matrix\n",
    "    # ==============================\n",
    "    labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_csv = os.path.join(OUTPUT_DIR, \"confusion_matrix_raw.csv\")\n",
    "    pd.DataFrame(cm, index=labels, columns=labels).to_csv(cm_csv, encoding=\"utf-8\")\n",
    "    print(f\"[OK] Saved confusion matrix (raw counts) -> {cm_csv}\")\n",
    "\n",
    "    # Normalized (row-wise)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "        cm_norm = np.nan_to_num(cm_norm)\n",
    "    cmn_csv = os.path.join(OUTPUT_DIR, \"confusion_matrix_normalized.csv\")\n",
    "    pd.DataFrame(cm_norm, index=labels, columns=labels).to_csv(cmn_csv, encoding=\"utf-8\")\n",
    "    print(f\"[OK] Saved confusion matrix (normalized) -> {cmn_csv}\")\n",
    "\n",
    "    # ==============================\n",
    "    # Plot: Accuracy bar\n",
    "    # ==============================\n",
    "    acc_png = os.path.join(OUTPUT_DIR, \"accuracy.png\")\n",
    "    plt.figure(figsize=(5, 4), dpi=140)\n",
    "    plt.bar([\"Accuracy\"], [acc])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    for i, v in enumerate([acc]):\n",
    "        plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(acc_png, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved accuracy graph -> {acc_png}\")\n",
    "\n",
    "    # ==============================\n",
    "    # Plot: Confusion matrix heatmap (raw)\n",
    "    # ==============================\n",
    "    cm_png = os.path.join(OUTPUT_DIR, \"confusion_matrix_heatmap.png\")\n",
    "    fig = plt.figure(figsize=(max(6, 0.35*len(labels)), max(5, 0.35*len(labels))), dpi=140)\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(\"Confusion Matrix (Counts)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    # annotate\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            val = cm[i, j]\n",
    "            ax.text(j, i, str(val),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if val > thresh else \"black\",\n",
    "                    fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_png, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved confusion-matrix heatmap (counts) -> {cm_png}\")\n",
    "\n",
    "    # ==============================\n",
    "    # Plot: Confusion matrix heatmap (normalized)\n",
    "    # ==============================\n",
    "    cmn_png = os.path.join(OUTPUT_DIR, \"confusion_matrix_heatmap_normalized.png\")\n",
    "    fig = plt.figure(figsize=(max(6, 0.35*len(labels)), max(5, 0.35*len(labels))), dpi=140)\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(cm_norm, interpolation=\"nearest\")\n",
    "    plt.title(\"Confusion Matrix (Row-normalized)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    # annotate with percentages\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            val = cm_norm[i, j]\n",
    "            ax.text(j, i, f\"{val*100:.1f}%\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if val > 0.5 else \"black\",\n",
    "                    fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cmn_png, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved confusion-matrix heatmap (normalized) -> {cmn_png}\")\n",
    "\n",
    "# ==============================\n",
    "# OPTIONAL: Retrieval Recall@K\n",
    "# If your CSV has retrieval columns, we compute Recall@1/3/5 and plot a bar chart.\n",
    "# ==============================\n",
    "q_col = find_col(QUERY_COL_CAND, df.columns)\n",
    "g_col = find_col(GOLD_ID_CAND, df.columns)\n",
    "r_col = find_col(RETR_IDS_CAND, df.columns)\n",
    "\n",
    "def parse_id_list(x):\n",
    "    \"\"\"Accepts formats: JSON list, 'a,b,c', 'a|b|c'; returns list of strings.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # Try JSON/list literal\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            return [str(z) for z in v]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: split by comma or pipe\n",
    "    if \"|\" in s:\n",
    "        return [t.strip() for t in s.split(\"|\") if t.strip()]\n",
    "    if \",\" in s:\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    return [s]\n",
    "\n",
    "if all(c is not None for c in [q_col, g_col, r_col]):\n",
    "    print(\"[INFO] Retrieval columns detected. Computing Recall@K...\")\n",
    "    golds = df[g_col].astype(str).fillna(\"\")\n",
    "    rets  = df[r_col].apply(parse_id_list)\n",
    "\n",
    "    def recall_at_k(k):\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        for g, lst in zip(golds, rets):\n",
    "            if not g or not lst:\n",
    "                continue\n",
    "            total += 1\n",
    "            if g in lst[:k]:\n",
    "                hits += 1\n",
    "        return (hits / total) if total > 0 else 0.0, total\n",
    "\n",
    "    ks = [1, 3, 5]\n",
    "    recalls = []\n",
    "    base_total = None\n",
    "    for k in ks:\n",
    "        r, total = recall_at_k(k)\n",
    "        recalls.append(r)\n",
    "        base_total = total if base_total is None else base_total\n",
    "\n",
    "    # Save table\n",
    "    recall_df = pd.DataFrame({\"K\": ks, \"Recall\": recalls})\n",
    "    recall_csv = os.path.join(OUTPUT_DIR, \"retrieval_recall_at_k.csv\")\n",
    "    recall_df.to_csv(recall_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] Saved retrieval Recall@K -> {recall_csv} (evaluated over {base_total} queries)\")\n",
    "\n",
    "    # Plot bar\n",
    "    recall_png = os.path.join(OUTPUT_DIR, \"retrieval_recall_at_k.png\")\n",
    "    plt.figure(figsize=(6, 4), dpi=140)\n",
    "    plt.bar([str(k) for k in ks], recalls)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Retrieval Recall@K\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    for i, v in enumerate(recalls):\n",
    "        plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(recall_png, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved retrieval Recall@K chart -> {recall_png}\")\n",
    "else:\n",
    "    print(\"[INFO] Retrieval columns not found; skipping Recall@K (optional).\")\n",
    "\n",
    "print(\"\\n✅ Done. Check your output folder:\")\n",
    "print(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd3612-70c3-46e3-ae86-21101ae61756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
